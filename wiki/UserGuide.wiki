[UserGuide#What_is_olap4cloud What is olap4cloud?]

[UserGuide#How_to_build How to build]

 * [UserGuide#Prerequisites Prerequisites] 

 * [UserGuide#Getting_the_code Getting the code]

 * [UserGuide#Build_the_code Build the code]

[UserGuide#Installing Installing]

 * [UserGuide#Prerequisites Prerequisites]

 * [UserGuide#Installing_olap4cloud Installing olap4cloud]

[UserGuide#Main_concepts Main concepts]

[UserGuide#Using_olap4cloud_API Using olap4cloud API]

=What is olap4cloud=

olap4cloud is Hadoop & HBase based OLAP engine. It constructed to serve the OLAP-like queries containing grouping and aggregations.

The typical query which can be served by olap4cloud looks like following: 

select d3, sum(m1), min(m2) from facts where d1 in (1,2) and d2 in (2,3) group by d3;

The main features of olap4cloud which allows to outperform the brute-force map/reduce competitors(Hive, Pig) are:
 * data defragmentation - olap4cloud stores data in the order where rows with the same dimensions will be stored closely. 
 * intensive indexing - olap4cloud constructs special indexes. These indexes helps to locate where data stored and avoid full scan during query execution.
 * preaggregations - olap4cloud uses classic lattice model to construct aggregation cuboids. It allows to achieve lower latency on queries where aggregations are already calculated.

=How to build=

At this time the olap4cloud project is in the highly experimental stage. So we don't have any stable builds yet, and recommended way to get the jar file is to get the fresh source code from SVN and build it.

==Prerequisites==

You need to have installed svn client, jdk 6 and ant on your computer in order to build the olap4cloud.

==Getting the code==

Obtain the code by using the one of SVN checkout commands listed on Source tab (you will need an SVN client installed on your computer).

==Build the code==

olap4cloud can be built by simply running the "ant" command in the root directory of the project. 
After build is done you will see the olap4cloud.jar file in the root directory of the project.

Other helpful ant targets are:
- clean - remove all compiled java classes and jars.
- test.build - build tests
- test.generate_test_data - generate test data for tests. Data files will be located under test/data directory. 

=Installing=

==Prerequisites==

You need to have Hadoop and HBase up and running on your cluster. 

Additionally olap4cloud intensively uses Hadoop & HBase Map/Reduce facility, so you need to have Hadoop and HBase configured to be able run Map/Reduce jobs against HBase tables. 
This part can be done using instructions available on the following address: http://hadoop.apache.org/hbase/docs/current/api/org/apache/hadoop/hbase/mapreduce/package-summary.html#classpath

==Installing olap4cloud==

You need to add olap4cloud.jar to Hadoop classpath, for example you can define HADOOP_CLASSPATH variable in hadoop-env.sh.

=Main concepts=

The central object of olap4cloud is OLAP cube(http://en.wikipedia.org/wiki/OLAP_cube). In other words the main goals of olap4cloud is to manage and query cubes. However at this moment olap4cloud doesn't support dimension tables and hierarchies, therefore cube is just fact table(http://en.wikipedia.org/wiki/Fact_table) for now. Fact table contains dimensions and measures. At this moment all dimensions can have only java type long and measures should have java type double.

The typical life cycle of cube is:
 * Load data to cube using ETL module.
 * Perform queries against the cube.

=Using olap4cloud API=

Currently olap4cloud provides only the Java API. The client program should have access to Hadoop Map/Reduce facility (i.e. it can be run by 'hadoop jar' command).

The entry point of the olap4cloud API is OLAPEngine class. It provides the methods for cube manipulation and for query execution.

==!CubeDescriptor==

!OLAPEngine methods require !CubeDescriptor object which contains definition of cube properties.

!CubeDescriptor class defines cube and their properties. 

The properties of cube defined by !CubeDescriptor includes:

 * cube name
 * data source directory - points to the HDFS directory where loaded into cube data should be located. It is used in ETL process.
 * measures - measures of fact table. 
 * dimensions - dimensions of fact table. The order of measures very critical: the measures which are used in queries more often should be located at the beginning of the list. I.e. if each query contains dimension 'productId', it should definitely be located at first place of the dimensions list.

There are two ways to define !CubeDescriptor object:
 * in xml file(recommended way)
 * in java code